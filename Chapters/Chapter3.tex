% Chapter Template

\chapter{Related Work} % Main chapter title

\label{Chapter3:RelatedWork} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1 4+ pages
%----------------------------------------------------------------------------------------
In this chapter we discuss about various recent state of the art approaches and methods for estimating Monocular depths using deep neural networks (DNNs). We will be discussing about different earlier approaches on this subject in section \ref{Chapter3:RelatedWork_EarlyApproach} briefly and look into more specific and elaborated details in recent approaches using Neural Networks based on different important factors in section \ref{Chapter3:RelatedWork_NNModel}. We have also discussed about various datasets which are available. 

\section{Early Approaches}
\label{Chapter3:RelatedWork_EarlyApproach}
Meanwhile understating of structural orientation, recovering range and object depicted in an given image is one of the basic problems of it. Many traditional approaches mainly focus on low-level image cues and geometric structures - most often handcrafted method \cite{torralba2002depth, pentland1987new,lai1992generalized,saxena2006learning}.
 
One of the very early approaches on image understanding with respect to reconstruction of depth image can be traced back till 1982 Barnard et. al. \cite{barnard1982computational} where they discuss various state of the art  computational methods for recovery of depth images at that period of time, which mainly focus on two aspects which are camera geometry and disparity mapping. Later in 1987 Stephen T. Barnard proposed a stochastic approach which provides a dense array of disparities, eliminating the need for interpolation \cite{barnard1987stochastic}. Later D. Scharstein and R. Szeliski \cite{scharstein2002taxonomy} came up with a taxonomy for dense two-frame stereo correspondence algorithms.\\

\subsection{Probabilistic Models}
\label{Chapter3:RelatedWork_ProbabilisticModel}
Probabilistic Models were the initial steps towards DNNs. Sexana et al. \cite{saxena2006learning} in 2006 were one of the first to propose an probabilistic model to predict depth image from single image. This approach was based on Markov Random Field (MRF) using Gaussian and Laplasian distribution model. Features where considered on small patch level of a given image \footnote{images were sub divided into small patches} in two distinctive levels. First, to estimate the absolute depth and second to estimate relative depth. This relative depth were calculated based on magnitude of the difference in depth between two patches for which 3 different local cues such as texture variations, texture gradients and haze were considered. Distance 3D scanner where used for data collection for this purpose. Their work was extended to 3D scene reconstruction with improved MRF model, Make3D \cite{saxena2008make3d} system for 3D model generation. One of the challenges of this system is that the images relies on horizontal allied calibration. 

This work lead to various new probabilistic models which in recent years can be classified into Deep Neural Networks (DNNs). Recent years many solutions has be tried for monocular depth estimation problem such as supervised, semi-supervised and unsupervised learning.   

One of the inspiring modern approach by Eigen et al. 2014 \cite{eigen2014depth} where two network component stack were used namely global coarse-scale network followed by local fine-scale network. Global coarse-scale network predict overall depth structure which is intern then refined by a local fine-scale network. This opened doors for fusing feature maps from different level for better predictions.
Also many Convolution Neural Networks (CNNs) based models where used to understand the relationship between RGB images and its corresponding depth maps \cite{liu2015deep,laina2016deeper,Eigen_2015_ICCV,eigen2014depth, Alhashim2018}. By this time encoder-decoder style of architecture where famous\cite{Alhashim2018, hu2019revisiting} which we will discuss in detail in section \ref{Chapter3:RelatedWork_NNModel}. 

Meanwhile some other works focused of various other details like,  Ladicky et al. \cite{ladicky2014pulling} highlights the limitation of the various data driven approaches for monocular depth estimation by exploring the structural perspective  geometry. Zammir et al. \cite{zamir2018taskonomy} studied on the modeling structure space of visual task and investigating on transfer learning dependencies, one of the finding where, the demand of labeled dataset could be reduced by introducing transfer learning approaches since there is always model and structural dependencies. Ha et al. \cite{ha2016high} also proposed a high quality depth map from non calibrated short video clip. Shu et al. \cite{Shi2015BreakAR} proposed that small-scale de focus blur can enhance the depth prediction and Fouhey et al. \cite{Fouhey_2013_ICCV} also tried to learn structural component.




\section{Recent Neural network Approach}
\label{Chapter3:RelatedWork_NNModel}
 Remarkable advances has been made in deep learning Laina et al. \cite{laina2016deeper} proposed a new architecture build on ResNet-50 by replacing the last layer with up sampling layers for reconstructions. They also proposed new loss function \textit{Huber Loss} an end to end approach, model can learn geometrical relationship. In the same way some approaches replaced ResNet backbone by different pretrained models as encoder\cite{Alhashim2018, hu2019revisiting}. More recent work focused on combining information from multiple scale, encoder and decoder style. This is to get the different level learned features \cite{Xu_2018_CVPR, Eigen_2015_ICCV} then concatenated at the decoder part of up sampling stage of architecture. One of main reason for such approach is to get higher spatial resolution by eliminating distorted and blurry edge since the probabilistic distribution always results into smooths object boundaries\cite{hu2019revisiting}. Also having features learned from top layers contains higher level information like which can give a global understanding of structural aspect of a image or scene. Meanwhile there where also some unsupervised learning methods applied for the same task \cite{godard2017unsupervised, qi2018geonet}.

We have analyzed the resent work in detail and will be describing more about different approaches categorizing into three groups model architecture, input representation and dataset used. These are the most commonly known to be the most important factors which defines the performance of a Network.

\subsection{Model Architecture}
\label{Chapter3:ModelArch}
In recent years since many encoder-decoder and multi scale style proven to give better results \cite{Alhashim2018, hu2019revisiting}. In this work we have used the similar architecture. In these encoder-decoder architecture the encoder always comprises of a backbone of a larger pretrained model. The most common encoder backbone which could be found are Residual Network (ResNet), Densely Connected Convolutional Network (DenseNet), Squeeze and Excitation Network (SENet) or Visual Geometry Group Network (VGGNet)\cite{hu2019revisiting}. Very deep neural network are difficult to train due to vanishing and exploding gradient. ResNet helps to skip intermediate identity connections by 
\begin{equation} \label{eqResNet}
    {a^{i+n}=g(z^{i+n} + a^i)}
\end{equation}


where \(g\) is the non-linear activation (eg. ReLu) and \(z\) is the output of linear activation (or output of a particular layer) of $i^{\text{th}}$ layer and \(a\) denotes the output of a layer. In contrast, DenseNet is an extension of ResNet where instead of skipping the and merging with the $i+n^{\text{th}}$ layer as an addition, DenseNet performs concatenation of all the $n$ skipped feature maps
\begin{equation} \label{eqDenseNet}
    {a^{i+n}=g(z^{i+n} + a^{C})}
\end{equation}


where \({C =C_i([i,i_2,...,i_{n-1}])}\) denotes the concatenation of layers till previous layer \(i_{n-1}^{th}\)  using the summation \cite{huang2017densely} there by helping th feature propagation, feature reuse and  reduce the parameters. Where as SENet is a transforms a set of block to another. It comprises of \textit{squeeze} operator and \textit{excitation} operator.\textit{squeeze} operator aggregates the feature map and \textit{excitation} operator  aggregates the learned activations Net \cite{iandola2016squeezenet}. 


In table \ref{table:RelatedWork_STA_Architecture} based on the various neural network model architecture we compare with recent 6 different architecture which are mentioned. These models are selected based on the most recent approaches from past one year and sorted based on the Root Mean Squared Error (RMSE). All the RMSE score are based on test set of NYU v2 depth map dataset \cite{silberman11indoor}.

\textbf{M1}, \textbf{M3} and partially \textbf{M4}, were build upon ResNet backbone for depth prediction. Even though all the three methods are different, \textbf{M3} and \textbf{M4} methods have one thing in common which is that both architecture where designed for multi-task specific model after encoder-decoder part. \textbf{M3} Pattern Affinitive Propagation (PAP-Depth) method idea was developed based on the affinity behaviour between two similar task \footnote{for example finding Surface normal and depth are related task, These two factors remains the  most important component for image segmentation}. This can be described by Affinity Block or sub network, affinity block comes after the up-sampling (decoder part) the last layers are fed into three different task-specific networks for prediction of Depth, surface normal and segmentation. They also integrate multi-scale features derived from different layers of encoder with each task-specific network. Each task-specific network has two residual blocks, and produces the initial prediction after a convolution layer. Then conduct cross-task propagation to learn the task-level affinitive patterns. Each task-specific network firstly learns an affinity matrix by the affinity learning layer to capture the pair-wise similarities for each task.

\textbf{M3}, SharpNet basically was built to after encoder-decoder style for addressing pixel-perfect near occluding contours  problem. The approach of SharpNet is very similar \textbf{M4}. Instead of segmentation sub-net SharpNet has  occluding contours as on of the 3 multi-task network model after decoder part. The features from encoder in integrated to this sub-network. 

\textbf{M1}, Geometric Network (GeoNet) is a two layer architechure - built upon two two big CNN model ResNet and VGG in two different block. First block is two models (ResNet and VGG) are trained separately. VGG is used for depth images and ResNet for surface normal prediction of a image. The output is given to second block which consiste of two sub network namely \textit{Depth-to-Normal} and \textit{Normal-to-Depth}. As the names States the Depth output from VGG is fed to  \textit{Depth-to-Normal} to get refined depth and vice version to get refined Normal

\textbf{M2} and \textbf{M6} are built upon DenseNet backbone. In \textbf{M6} one of the distinctive idea is, the decoder part has multiple resolution block. If \(\textbf{\textit{$B_n$}}\) is denoted as one block of decoder to obtain output resolution of \(\textbf{\textit{$D_n$}}\) , Then  \(\textbf{\textit{$D$}}\) depends on number of  \(\textbf{\textit{B}}\). In this approach they have 5 different resolution \(\textbf{\textit{$D_n$}}\)  such that \(n = 5\) for extraction of multi resolution depth from different positions to get relative depths. Final depth image is obtained from relative pair-vise comparison with the relative depth images from different \(\textbf{\textit{$B_n$}}\)

\textbf{M2} DenseDepth is a much simpler model than all the above approaches yet proven to have best results than others in terms of RMSE on NYU dataset. There are two distictive  ideas. First, they use multi-scale encoder featured concatenated with decoder part to get a better strutural higher level features, also the decoder is made of \(2 x \) bilinear up sampling method. Secound, they use transfer learning approach.  

\begin{table}[t]
\centering
\begin{tabular}{p{0.05\linewidth}p{0.2\linewidth}p{0.1\linewidth}p{0.3\linewidth}p{0.2\linewidth}}
%{|c|c|c|c|c|}


\hline
\textbf{\# } & \textbf{Method} & \textbf{RMSE} & \textbf{Backend}& \textbf{Year/Reference} \\ \hline\hline
M1              & GeoNet          & 0.445             & ResNet and VGG  & 2018 \cite{qi2018geonet}          \\ \hline
M2              & DenseDepth      & 0.465             & DenseNet        & 2018 \cite{Alhashim2018}         \\ \hline
M3              & SharpNet        & 0.496             & ResNet       & 2019 \cite{ramamonjisoa2019sharpnet}\\ \hline
M4              & PAP-Depth       & 0.497             & ResNet          & 2019 \cite{Zhang_2019_CVPR}         \\ \hline
M5              & SENet-154       & 0.530             & SENet           & 2018 \cite{hu2019revisiting}          \\ \hline
M6              & RelativeDepth   & 0.538             & DenseNet        & 2019 \cite{lee2019monocular}          \\ \hline
\end{tabular}

\caption{Investigated neural network architecture on Monocular Depth Estimation on NYUv2 Dataset}
\label{table:RelatedWork_STA_Architecture}

\end{table}


There are some advantages of using DenseNet over ResNets. One of the first advantages is there is a strong gradient flow because increase in the depth of a CNN might result in vanishing gradient problem . Second, we get more diversified feature, which means there can be good generalized information from the previous layers which tends to have richer patterns where as in skip style ResNet such information are lost. 

In summary, despite of having simpler model by Alhashim et. al \cite{Alhashim2018}  the results seems convinsing when compaired with various other complex and big models. Having found the most common encoder backend for encoder-decoder style nerwork are  ResNet and DenseNet. These both have proven to to have state-of-art results. Inspired by the simplicity and the results of  Alhashim et. al. work on DenseDepth we use DenseNet model as out backbone encoder in our work for estimating depth maps. We would like to highlight three distinctive reason for selecting thing approach. First as we see DenseNet have proven to have some advantages over for deeper network. Second the model uses transfer learning and trained 120K images from NYU v2 datased which weights have be shared to public Which reduces the computation needed for re training this architecture again. Third, many recent models have verious multi-sub task network for their work, in our case we are more focused in structure sensor as described in section \ref{Chapeter1:Topic_Description} 


\subsection{Input Representation}
One of the important aspect of feature based learning is input representation of a feature to its respective model \cite{friedman2001elements}. The most common approach of input representation is in its simple form of RGB and depth pair from various SLB sensors \cite{eigen2014depth, xu2017multi}. Some approaches also exploits the structural orientation by computing surface normal \cite{li2015depth, qi2018geonet}. One of the approach Zhang et. al. \cite{Zhang_2019_CVPR} was to utilize three features  depth, surface normal and segmentation fused together by concatenation and given as input at encoder and at the decoder part a multi-sub task specific network was designed. The advantage of such multi feature is it can have a good cross task learning - which means different feature representation for same temporal content. One more example of  cross task learning is \cite{qi2018geonet} where two model where used for two different input feature depth image and surface normal. Another type of input which was used in some approaches is sparsity  \cite{chen2018estimating, mal2018sparse}. Chen et. al. \cite{chen2018estimating} computed the sparsity matrix and given as addition input along with depth maps. In our work we used the most common approach RGB - depth, one of the map pair as input feature.  

\subsection{Existing Datasets}

By now we have a understanding of various Neural Network methods used to approach this study of depth estimation from previous Sections. One of the most important component in feature based learning methods are highly dependent on data itself \cite{friedman2001elements}, which mean the result of such probabilistic output models can be highly influenced by the dataset itself. For example, We can either fit a model by training with the small amount of dataset to perform few particular designated task in a designed environment or we can train a model on a large and diverse database to perform more generalized tasks.

There are several datasets readily available for different purposes. One of the major classification of dataset for depth maps can be classified as either by the environment scene - indoor and outdoor environment or based on application, for example KITTI for Autonomous driving \cite{Geiger2013IJRR} scenario and Berkeley 3-D Object Dataset (B3DO) \cite{Janoch:EECS-2012-85} is soulely focused for Object Recognition. Here we categorize on the basis of the process of producing the dataset.

Saxena et al. \cite{saxena2006learning} proposed a dataset of 959 Images of RGB and laser range data. All the depth data was collected using a custom-built 3D laser scanner. The images are are of dimensions 2272 $\times$ 1704, while the depth maps are 55 $\times$ 305. Later, Silberman et al. \cite{Silberman:ECCV12} proposed a high quality Kinect dataset (NYU Depth-V2) in 2012 which is now being used widely across the in 2012. NYU Depth-V2 \cite{Silberman:ECCV12} consist of 1449 densely labeled pairs of aligned RGB and Depth images. By labeled pair we refer to segmentation of all the surfaces with respected surface normal. Furthermore, it consists 407024 unlabeled frames. Former NYU dataset(NYU Depth V1) \cite{silberman11indoor} consisted of only 67 scenes while NYU Depth-V2 consist of 464 different indoor scenes. Both the images and Depth maps are 640 $\times$ 480 in resolution. The dataset is appreciated for the segmentation of a room.

Geiger et al. \cite{Geiger2013IJRR} in 2013, proposed an outdoor dataset consisting of stereo depth images as The KITTI Dataset. KITTI Dataset is used for Autonomous Driving and Robotics purpose. It includes high resolution color and grayscale stereo camera images, laser scans, high-precision Global Positioning System(GPS) measurements and SLAM data. The main intend was to push forward the development of computer vision and robotic algorithms targeted to autonomous driving.

In 2016, Mayer et al. \cite{MIFDB16} produced three synthetic datasets called Scene Flow Dataset providing over 35000 stereo frames with dense ground truth for optical flow, disparity and disparity change, as well as other data such as object segmentation. The resolution of these images is 960 $\times$ 540. MPI Sintel \cite{Butler:ECCV:2012} is also a synthetic depth dataset which is available online. All these dataset were created using the open source 3D creation suite Blender.

Another traditional approach to collect dataset is using crowd-sourcing. Chen et al. \cite{DBLP:journals/corr/ChenFYD16} introduced "Depth in the Wild" dataset in which they took images from Flickr \footnote{ \url{www.flickr.com}}  and presented the crowd-sourced workers with these images with two highlighted points asking which of the point was closer. The dataset consists of 495,000 diverse images, each annotated with randomly sampled points and their relative depth. 

\begin{table}[]
\begin{tabular}{lllllll}
\hline
{\textbf{Dataset}} & {\textbf{Approach Used}} & \textbf{Environment}        & \textbf{RGB resolution}           & \textbf{Depth resolution} & \textbf{Number of images} & \textbf{References}                                  \\ \hline
Make3D                                & Custom Laser Scanner                      & Outdoor                     & 2272 $\times$ 1704                & 55 $\times$ 305           & 959                       & http://make3d.cs.cornell.edu/data.html               \\ \hline
NYU V2                                 & Kinect V2                                   & Indoor                      & 640 $\times$ 480                  & 512 $\times$ 424          & 407,024                   & \textbackslash{}cite\{Silberman:ECCV12\}             \\ \hline
KITTI                                  & Stereo Imaging                              & Outdoor(Autonomous Driving) & 1392 $\times$ 512 (Original Size) & -                         & 15,000                    & \textbackslash{}cite\{Geiger2013IJRR\}               \\ \hline
B3DO                                   & Kinect V2                                   & Indoor                      & 640 $\times$ 480                  & 512 $\times$ 424          & 849                       & \textbackslash{}cite\{Janoch:EECS-2012-85\}          \\ \hline
Cornell Dataset                        & Kinect                                      & Indoor(Robotics)            & 640 $\times$ 480                  & 640 $\times$ 480          & 550                       & \textbackslash{}cite\{3Dscene\}                      \\ \hline
Washington V2                          & Kinect                                      & Indoor(Object Recognition)  & 640 $\times$ 480                  & 640 $\times$ 480          & 300                       & \textbackslash{}cite\{Washington\}                   \\ \hline
Scene Flow Dataset                     & Synthetic/Stereo                            & Outdoor                     & 960 $\times$ 540                  & 960 $\times$ 540          & 39000                     & \textbackslash{}cite\{MIFDB16\}                      \\ \hline
Depth in the Wild                      & Crowd Sourcing                              & -                           & -                                 & -                         & 495,000                   & \textbackslash{}cite\{DBLP:journals/corr/ChenFYD16\} \\ \hline
\end{tabular}
\caption{Comparision of various Datasets}
\label{table:DatasetComparision}
\end{table}

There are many Other datasets are available like Cornell Dataset \cite{3Dscene} , Washington Data V2 \cite{Washington} and  Berkeley 3-D Object dataset (B3DO) \cite{Janoch:EECS-2012-85} also follows the same approach of labelling the environment either for object recognition or robotics purpose. All of them were also captured using Kinect camera approach producing RGB-D images. We can see the comparison of all the datasets in table \ref{table:DatasetComparision}.

As we can see, all of the Datasets mentioned above are suitable for different application scenarios. For example, NYU V2 \cite{Silberman:ECCV12} is best for Segmentation purpose, B3DO \cite{Janoch:EECS-2012-85} is suitable for Object Recognition and KITTI Dataset \cite{Geiger2013IJRR} for Autonomous Driving purpose. Keeping in mind the scope of the research is to estimate depth of monocular indoor scenarios and office environments, NYU V2 is the most relevant as it has high quality depth maps for indoor environment which can be exploited for training of the network. As discussed in section \ref{Chapter3:ModelArch}, we produce a small dataset using a Structure Sensor to make the model more focused towards mobile devices which is discussed in next chapter.



%\section{Additional Details}
%\url{https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2?p=high-quality-monocular-depth-estimation-via}%
%upsampling cound be in many parts

%\begin{itemize}
 %  \item Nearest Neighbor
  % \item Bilinear Interpolation,  A single pixel value is calculated as the weighted avg.
   %\item Transposed Convolution, we have weights that we learn through back propagation.
%   \item Recent works
%\end{itemize}


%Supervised 
%CNN - slam - \cite{Tateno_2017_CVPR}
%CRNN
%GeoNet

%\\
%unsupervised \\
%un supersed CNN - geometry  - \cite{garg2016unsupervised}
% Unsupervised Learning of Depth and Ego-Motion From Video- \cite{Xu_2018_CVPR}
%Left and right

%\\
%Transfer learning\\
%CNN -

%\\
%{https://github.com/GabrielMajeri/nyuv2-python-toolbox}\\
%{https://github.com/ayanc/mdepth}\\
%Some \cite{bhoi2019monocular} \cite{laina2016deeper, bhoi2019monocular}\\
%We use this curretn state of the art 
