% Chapter Template

\chapter{Related Work} % Main chapter title

\label{Chapter3:RelatedWork} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1 4+ pages
%----------------------------------------------------------------------------------------
In this chapter we discuss about various recent state of the art approaches and methods for estimating Monocular depths using deep neural networks (DNNs). We will be discussing about different earlier approaches on this subject in section \ref{Chapter3:RelatedWork_EarlyApproach} briefly and look into more specific and elaborated details in recent approaches using Neural Networks based on different important factors in section \ref{Chapter3:RelatedWork_NNModel}. 

\section{Early Approaches}
\label{Chapter3:RelatedWork_EarlyApproach}
Meanwhile understating of structural orientation, recovering range and object depicted in an given image is one of the basic problems of it. Many traditional approaches mainly focus on low-level image cues and geometric structures \cite{torralba2002depth, pentland1987new,lai1992generalized,saxena2006learning}.
 
One of the very early approaches to our knowledge on image understanding with respect to reconstruction of depth image can be traced back till 1982 Barnard et al. \cite{barnard1982computational} where they discuss various state of the art  computational methods for recovery of depth images, which mainly focus on two aspects camera geometry and disparity mapping. Later in 1987 Stephen T. Barnard proposed a stochastic approach which provides a dense array of disparities, eliminating the need for interpolation \cite{barnard1987stochastic}. Later D. Scharstein and R. Szeliski \cite{scharstein2002taxonomy} came up with a taxonomy for dense two-frame stereo correspondence algorithms. \nadacn{All more may be 5 lines..}\\

\subsection{Probabilistic Models}
\label{Chapter3:RelatedWork_ProbabilisticModel}
Probabilistic Models where the initial steps towards DNNs. Sexana et al. \cite{saxena2006learning} in 2006 were one of the first to propose an probabilistic model to predict depth image from single images. This approach was based on Markov Random Field (MRF) using Gaussian and Laplasian distribution model. Features where considered on small patch level of a given image \footnote{images were sub divided into small patches} in two levels. First, to estimate the absolute depth and second to estimate relative depth. This relative depth were calculated based on magnitude of the difference in depth between two patches for which 3 different local cues such as texture variations, texture gradients and haze where considered. Distance 3D scanner where used for data collection for this purpose. Their work was extended to 3D scene reconstruction with improved MRF model, Make3D \cite{saxena2008make3d} system for 3D model generation. One of the challenges of this system is that the images relies on horizontal allied calibration. 

This work lead to various new probabilistic models which in recent years can be classified into Deep Neural Networks (DNNs). Recent years many solutions has be tried for monocular depth estimation problem such as supervised, semi-supervised and unsupervised learning.   

One of the inspiring modern approach by Eigen et al. 2014 \cite{eigen2014depth} where two network component stack were used namely global coarse-scale network followed by local fine-scale network. Global coarse-scale network predict overall depth structure which is intern then refined by a local fine-scale network. This opened doors for fusing feature maps from different level for better predictions.
Also many Convolution Neural Networks (CNNs) based models where used to understand the relationship between RGB images and its corresponding depth maps \cite{liu2015deep,laina2016deeper,Eigen_2015_ICCV,eigen2014depth, Alhashim2018}. By this time encoder-decoder style of architecture where famous\cite{Alhashim2018, hu2019revisiting} which we will discuss in detail in section \ref{Chapter3:RelatedWork_NNModel}. 

Meanwhile some other works focused of various other details like,  Ladicky et al. \cite{ladicky2014pulling} highlights the limitation of the various data driven approaches for monocular depth estimation by exploring the structural perspective  geometry. Zammir et al. \cite{zamir2018taskonomy} studied on the modeling structure space of visual task and investigating on transfer learning dependencies, one of the finding where, the demand of labeled dataset could be reduced by introducing transfer learning approaches since there is always model and structural dependencies. Ha et al. \cite{ha2016high} also proposed a high quality depth map from non calibrated short video clip. Shu et al. \cite{Shi2015BreakAR} proposed that small-scale de focus blur can enhance the depth prediction and Fouhey et al. \cite{Fouhey_2013_ICCV} also tried to learn structural component.

Insspired from Ladicky et al. and Zammir et al. work 



\section{Recent Neural network Approach}
\label{Chapter3:RelatedWork_NNModel}
 Remarkable advances has been made in deep learning Laina et al. \cite{laina2016deeper} proposed a new architecture build on ResNet-50 by replacing the last layer with up sampling layers for reconstructions. They also proposed new loss function \textit{Huber Loss} an end to end approach, model can learn geometrical relationship. In the same way some approaches replaced ResNet backbone by different pretrained models as encoder\cite{Alhashim2018, hu2019revisiting}. More recent work focused on combining information from multiple scale, encoder and decoder style. This is to get the different level learned features \cite{Xu_2018_CVPR, Eigen_2015_ICCV} then concatenated at the decoder part of up sampling stage of architecture. One of main reason for such approach is to get higher spatial resolution by eliminating distorted and blurry edge since the probabilistic distribution always results into smooths object boundaries\cite{hu2019revisiting}. Also having features learned from top layers contains higher level information like which can give a global understanding of structural aspect of a image or scene. In this section we will understand the different works done with Neural network analyzing different components of it.

We have analyzed the resent work in detail and will be describing more about different approaches based on 4 factors namely, model architecture, loss functions and input representation and dataset used. These are the most commonly known to be the most important factors which defines the performance of a Network.  

\subsection{Model Architecture}
\label{Chapter3:ModelArch}
I recent years since many encoder-decoder and multi scale style proven to give better results \cite{Alhashim2018, hu2019revisiting}. In this work we have used the similar architecture. In these encoder-decoder architecture the encoder always comprises of a backbone of a larger pretrained model. The most common encoder backbone which could be found are Residual Network (ResNet), Densely Connected Convolutional Network (DenseNet), Squeeze and Excitation Network (SENet) or Visual Geometry Group Network (VGGNet)\cite{hu2019revisiting}. Very deep neural network are difficult to train due to vanishing and exploding gradient. ResNet helps to skip intermediate identity connections by 
\begin{equation} \label{eqResNet}
    {a^{i+n}=g(z^{i+n} + a^i)}
\end{equation}


where \(g\) is the non-linear activation (eg. ReLu) and \(z\) is the output of linear activation (or output of a particular layer) of $i^{\text{th}}$ layer and \(a\) denotes the output of a layer. In contrast, DenseNet is an extention of ResNet where instead of skipping the and merging with the $i+n^{\text{th}}$ layer as an addition, DenseNet performs concatenation of all the $n$ skipped feature maps
\begin{equation} \label{eqDenseNet}
    {a^{i+n}=g(z^{i+n} + a^{C})}
\end{equation}


where \({C =C_i([i,i_2,...,i_{n-1}])}\) denotes the concatenation of layers till previous layer \(i_{n-1}^{th}\)  using the summation \cite{huang2017densely} there by helping th feature propagation, feature reuse and  reduce the parameters. Where as SENet is a transforms a set of block to another. It comprises of \textit{squeeze} operator and \textit{excitation} operator.\textit{squeeze} operator aggregates the feature map and \textit{excitation} operator  aggregates the learned activations Net \cite{iandola2016squeezenet}. 


In table \ref{table:RelatedWork_STA_Architecture} based on the various neural network model architecture we compare with recent 6 different architecture which are mentioned. These models are selected based on the most recent approaches from past one year and sorted based on the Root Mean Error (RMSE). All the RMSE score are based on test set of NYU v2 depth map dataset \cite{silberman11indoor}.

\textbf{M1}, \textbf{M3} and partially \textbf{M4}, were build upon ResNet backbone for depth prediction. Even though all the three methods are different, \textbf{M3} and \textbf{M4} methods have one thing in common which is that both architecture where designed for multi-task specific model after encoder-decoder part. \textbf{M3} Pattern Affinitive Propagation (PAP-Depth) method idea was developed based on the affinity behaviour between two similar task \footnote{for example finding Surface normal and depth are related task, These two factors remains the  most important component for image segmentation}. This can be described by Affinity Block or sub network, affinity block comes after the up-sampling (decoder part) the last layers are fed into three different task-specific networks for prediction of Depth, surface normal and segmentation. They also integrate multi-scale features derived from different layers of encoder with each task-specific network. Each task-specific network has two residual blocks, and produces the initial prediction after a convolution layer. Then conduct cross-task propagation to learn the task-level affinitive patterns. Each task-specific network firstly learns an affinity matrix by the affinity learning layer to capture the pair-wise similarities for each task.

\textbf{M3}, SharpNet basically was built to after encoder-decoder style for addressing pixel-perfect near occluding contours  problem. The approach of SharpNet is very similar \textbf{M4}. Instead of segmentation sub-net SharpNet has  occluding contours as on of the 3 multi-task network model after decoder part. The features from encoder in integrated to this sub-network. 

\textbf{M1}, Geometric Network (GeoNet) is a two layer architechure - built upon two two big CNN model ResNet and VGG in two different block. First block is two models (ResNet and VGG) are trained separately. VGG is used depth and ResNet for surface normal. The output is given to second block which consiste of two sub network namely \textit{Depth-to-Normal} and \textit{Normal-to-Depth}. As the names States the Depth output from VGG is fed to  \textit{Depth-to-Normal} to get refined depth and vice version to get refined Normal

\textbf{M2} and \textbf{M6} are built upon DenseNet backbone. \textbf{M6} \nadacn{have three distinctive ideas. First,} the decoder part has multiple resolution block. If \(\textbf{\textit{$B_n$}}\) is denoted as one block of decoder to obtain output resolution of \(\textbf{\textit{$D_n$}}\) , Then  \(\textbf{\textit{$D$}}\) depends on number of  \(\textbf{\textit{B}}\). In this approach they have 5 different resolution \(\textbf{\textit{$D_n$}}\)  such that \(n = 5\) for extraction of multi resolution depth from different positions to get relative depths. Final depth image is obtained from relative pair-vise comparison with the relative depth images from different \(\textbf{\textit{$B_n$}}\)

\textbf{M2} DenseDepth is a much simpler model than all the above approaches yet proven to have best results than others in terms of RMSE on NYU dataset. There are two distictive  ideas. First, they use multi-scale encoder featured concatenated with decoder part to get a better strutural higher level features, also the decoder is made of \(2 x \) bilinear up sampling method. Secound, they use transfer learning approach.  

\begin{table}[t]
\centering
\begin{tabular}{p{0.05\linewidth}p{0.2\linewidth}p{0.1\linewidth}p{0.3\linewidth}p{0.2\linewidth}}
%{|c|c|c|c|c|}


\hline
\textbf{\# } & \textbf{Method} & \textbf{RMSE} & \textbf{Backend}& \textbf{Year/Reference} \\ \hline\hline
M1              & GeoNet          & 0.445             & ResNet and VGG  & 2018 \cite{qi2018geonet}          \\ \hline
M2              & DenseDepth      & 0.465             & DenseNet        & 2018 \cite{Alhashim2018}         \\ \hline
M3              & SharpNet        & 0.496             & ResNet       & 2019 \cite{ramamonjisoa2019sharpnet}\\ \hline
M4              & PAP-Depth       & 0.497             & ResNet          & 2019 \cite{Zhang_2019_CVPR}         \\ \hline
M5              & SENet-154       & 0.530             & SENet           & 2018 \cite{hu2019revisiting}          \\ \hline
M6              & RelativeDepth   & 0.538             & DenseNet        & 2019 \cite{lee2019monocular}          \\ \hline
\end{tabular}

\caption{Investigated neural network architecture on Monocular Depth Estimation on NYUv2 Dataset}
\label{table:RelatedWork_STA_Architecture}

\end{table}


There are some advantages of using DenseNet over ResNets. One of the first advantages is there is a strong gradient flow because increase in the depth of a CNN might result in vanishing gradient problem . Second, we get more diversified feature, which means there can be good generalized information from the previous layers which tends to have richer patterns where as in skip style ResNet such information are lost. 

In summary, despite of having simpler model by Alhashim et. al \cite{Alhashim2018}  the results seems convinsing when compaired with various other complex and big models. Having found the most common encoder backend for encoder-decoder style nerwork are  ResNet and DenseNet. These both have proven to to have state-of-art results. Inspired by the simplicity and the results of  Alhashim et. al. work on DenseDepth we use DenseNet model as out backbone encoder in our work for estimating depth maps. We would like to highlight two distinctive reason for selecting thing approach. First as we see DenseNet have proven to have some advantages over for deeper network. Second the model uses transfer learning and trained 120K images from NYU v2 datased which weights have be shared to public Which reduces the computation needed for re training this architecture again. Third, many recent models have verious multi-sub task network for their work, in our case we are more focused in structure sensor as described in section \ref{Chapeter1:Topic_Description}


\subsection{Loss Functions }
great paper for it: Revisiting Single Image Depth Estimation:
Toward Higher Resolution Maps with Accurate Object Boundaries \cite{hu2019revisiting}

One loss funtion to be remembered PAP-Depth \cite{PAP-Depth}




\subsection{Input Representation}
Write something about input

1. Spare normal 
2. depth

in our work we are 

\section{Existing Datasets}
There are several datasets readily available. Saxena et al. \cite{saxena2006learning} proposed a dataset of more than 1000 outdoor and about 50 indoor RGB and laser range data. All the depth data was collected using a custom-built 3D laser scanner. The images are 2272 $\times$ 1704 in resolution, while the depth maps are 55 $\times$ 305.\\

Silberman et al. \cite{Silberman:ECCV12} proposed a high quality Kinect dataset (NYU Depth-V2) in 2012 which is now being used widely across the globe. NYU Depth-V2 \cite{Silberman:ECCV12} consist of 1449 densely labeled pairs of aligned RGB and Depth images. Furthermore, it consists 407024 unlabeled frames. Former NYU dataset(NYU Depth V1) \cite{silberman11indoor} consisted of only 67 scenes while NYU Depth-V2 consist of 464 different indoor scenes. Both the images and Depth maps are 640 $\times$ 480 in resolution. The dataset is appreciated for the segmentation of a room.\\

Geiger et al. \cite{Geiger2013IJRR} in 2013, proposed an outdoor dataset consisting of stereo depth images as The KITTI Dataset. KITTI Dataset is used for Autonomous Driving and Robotics purpose. It includes high resolution color and grayscale stereo camera images, laser scans, high-precision GPS \notice{Global Positioning System} measurements and SLAM data. The main intend was to push forward the development of computer vision and robotic algorithms targeted to autonomous driving.\\

In 2016, Mayer et al. \cite{MIFDB16} produced three synthetic datasets providing over 35000 stereo frames with dense ground truth for optical flow, disparity and disparity change, as well as other data such as object segmentation. The resolution of these images is 960 $\times$ 540. MPI Sintel \cite{Butler:ECCV:2012} is also a synthetic depth dataset which is available online. All these dataset were created using the open source 3D creation suite Blender.\\

Another approach to collect dataset is using crowd-sourcing. Chen et al. \cite{DBLP:journals/corr/ChenFYD16} took images from Flickr\notice{plese give url for Flicker} \footnote{ \url{google.com}}  and presented the crowd-sourced workers with these images with two highlighted points asking which of the point was closer. The dataset consists of 495K diverse images, each annotated with randomly sampled points and their relative depth.\\

\textbf{Other datasets:} Cornell Dataset \cite{3Dscene} , Washington Data V2 \cite{Washington} and  Berkeley 3-D Object dataset (B3DO) \cite{Janoch:EECS-2012-85} also follows the same approach of labelling the environment either for object recognition or robotics purpose. All of them were also captured using same Kinect camera approach producing RGB-D images.\\

In summary, all of the Datasets mentioned above are suitable for different application scenarios. For example, NYU V2 \cite{Silberman:ECCV12} is best for Segmentation purpose, B3DO \cite{Janoch:EECS-2012-85} is suitable for Object Recognition and KITTI Dataset \cite{Geiger2013IJRR} for Autonomous Driving purpose. As we have discussed earlier, the scope of the research is to estimate depth of monocular indoor scenes and office environments from  images taken with Ipad, NYU V2 is the most relevant as it has high quality depth maps for indoor environment which can be exploited for training the network. But even NYU V2 does not fulfill the requirements of the research completely due to different camera structure and intrinsic features of Ipad and Kinect, we decide to create our own dataset using an IPAD and a Structure Sensor and \notice{use the transfer learning(discussed earlier) with the ground truths provided from NYU V2}  , then train on our dataset to make it compatible with Ipad.





%\section{Additional Details}
%\url{https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2?p=high-quality-monocular-depth-estimation-via}%
%upsampling cound be in many parts

%\begin{itemize}
 %  \item Nearest Neighbor
  % \item Bilinear Interpolation,  A single pixel value is calculated as the weighted avg.
   %\item Transposed Convolution, we have weights that we learn through back propagation.
%   \item Recent works
%\end{itemize}


%Supervised 
%CNN - slam - \cite{Tateno_2017_CVPR}
%CRNN
%GeoNet

%\\
%unsupervised \\
%un supersed CNN - geometry  - \cite{garg2016unsupervised}
% Unsupervised Learning of Depth and Ego-Motion From Video- \cite{Xu_2018_CVPR}
%Left and right

%\\
%Transfer learning\\
%CNN -

%\\
%{https://github.com/GabrielMajeri/nyuv2-python-toolbox}\\
%{https://github.com/ayanc/mdepth}\\
%Some \cite{bhoi2019monocular} \cite{laina2016deeper, bhoi2019monocular}\\
%We use this curretn state of the art 
