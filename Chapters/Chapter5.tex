
\chapter{Methodology}

\label{Chapter5:Methodology} 
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

In this chapter we will discuss the entire work flow and the methods used in this study. Starting from the different model architecture used. We used two two main architecture and with some configurations to answer our research questions. Also we will be discussing how the structural dataset was created and some pre processing applied in order to investigate on research question.


\section{System Overview}
We approached this problem of depth map estimation from monocular images to give end to end solution as much as possible using two CNN. As we discussed in the section \ref{Chapeter1:Topic_Description} our main focus was (a) to deliver a robust method and (b) regeneration of dead pixels or holes for better reconstruction of images. For this we consider two CNN models, first, we propose a model and this approach we call it as \textbf{A1} and the second model approach is a state of the art model and we call it as \textbf{A2}. To deliver a state of the art method and results we try to propose \textbf{A1} and validate against \textbf{A2}. In this section we briefly describe both our models and input representation used for the experiments. And further more in order to improve our results we also propose different configuration methods. These configuration methods and motivations are described in details in the following section \ref{Chapter5:Experimental_Setup}. 


\subsection{Input Representation}
For our study we use two datasets NYU\_v2 Depth \cite{silberman11indoor} whose depth information was obtained from kinet sensor and the other dataset was created as a part of this study from Structure sensor which is described in chapter \ref{Chapter4:Dataset}. The input dimension of target RGB images for both the approaches \textbf{A1} and \textbf{A2} are (480$\times$640$\times$3). Whereas input dimensions of ground truth depth image for \textbf{A1} is same as the target image  which is (480$\times$640$\times$1) while for \textbf{A2} is  (240$\times$320$\times$1) which is half of its target input resolution.

There where two pre- processing methods applied on Structure sensor one to train on holes and other with nearest interpolation method. Different input representation where used for different configuration which are described in details in the following section \ref{Chapter5:Experimental_Setup}.

\subsection{Approach 1 (A1) - U-Net Style Network}

\begin{figure}[t]
    \centering
    \includegraphics[width = 14cm, height = 9cm]{Figures/A1.png}
    \caption{U-Net architecture (\textbf{A1}). The arrow mark shows the concatenation of the higher level learnt feature from encoder to the upsamplng decoder part.}
    \label{fig:A1-U-NetArchetecture}
\end{figure}{}

U-Net (\textbf{A1}) architecture as shown in Fig \ref{fig:A1-U-NetArchetecture} is built upon simple idea of using upsampling layer or de-convolution layer  \footnote{not to be confused with the nomenclature, there are various names for de-convolution such as up-sampling layers or transposed convolution layer used by tensorflow and keras (\url{www.tensorflow.org/})} for up scaling the learnt feature and regeneration of depth maps. Hence the decoder block comprises of transposed 2D convolution layers which have the same dimension as the kernal size of encoder block. This means number of layers in encoder and decoder are the sample that's why the name U-Net. We use \textbf{A1} architecture since its easy to configure and understand how the up sample of network can be done. Another advantage is training such a small network is faster but it is a challenge is the network can learn the low level features. 

\textbf{A1} consists of 4 blocks of encoder and decoder part. Each block of encoder consists of 4 layers, 3 layers of 2D Convolutions and 2D max pooling layer of size 2$\times$2 at the end of each block. We designed the 3 convolutions layer with the increasing order of number of filter 4, 16 and 32 with kernel size of 3$\times$3. In the similar way in each decoder block we have 4 layers, starting with a up-sampling layer with kernel size if 2$\times$2 which means the up scaling of the image is in the factor of 2. Nearest neighbor interpolation method used by the up-sampling layer \footnote{\url{www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D}} implemented by keras layer. Towards the end each decoder block as 3 convolution 2D stacked in the similar way as encoder blocks. Each decoder block is concatenated with relative encoder block with has same dimension as shown in fig. \ref{fig:A1-U-NetArchetecture} with arrow marks. The dimension of encoder and decoder block output tensors are kept in symmetrical. This can be achieved by having same kernel size of max pooling at encoder part and up sampling layer at decoder, hence the symmetry in the emcoder and decoder layers. The last output layer comprises of Convolution 2D layer of 1$\times$1 with sigmoid activation. Also last layer has no stride to keep the dimension same as the input. 
This model has input dimension of (batch size$\times$480$\times$640$\times$3) and output dimension of (480$\times$640$\times$1) for both target RGB image and ground truth depth images.



\subsection{Approach 2 (A2) - DenseNet backbone}

\begin{figure}[h]
    \centering
    \includegraphics[width = 15cm,  height = 10cm]{Figures/A2.png}
    \caption{\textbf{A2} with DenseNet backbone}
    \label{fig:A2-DenseNet-arch}
\end{figure}{}




As shown in fig. \ref{fig:A2-DenseNet-arch} shows an overview of our second approach \textbf{A2} for depth estimation. We adapted the idea proposed by Alhashim et. al. \cite{Alhashim2018}. The basic idea of this approach \textbf{A2} was to use DenseNet as backbone or as an encoder from our literature study on this topic. The input RGB image is fed to the DenseNet-169 \cite{huang2017densely} network which is pretrained on ImageNet \cite{deng2009imagenet}.  Backbone DenseNet architecture is designed in a feed forward fashion within a dense block or in other words for feed forward design is that each layer is directly connected to every other layer. But since they adapt DenseNet style there is also skip connections between the layers. ImageNet is a large database of images which was build upon WorldNet which is organized in a hierarchical manner, which means the images are trained and clustered according to carious classes which give a good hierarchical tree and sub-tree format for classification or clustering. Another great advantage of this net is its versatility of the classes ranging from mammals, vehicles, birds to furniture with 12 subtree which gives us 5247 category at the time of this paper by Deng et. al.\ref{deng2009imagenet}. ImageNet claims to have a order of 50 million images and an average of 5000 image per node \footnote{\url{http://www.image-net.org/}}. ImageNet is also good for object recognition, classification and also clustering problems. Due to such versatility ImageNet can give a us a good generalisation of the physical structure from a 2D image which is very important for our task. This also has proven to give state of the results by the model proposed by Alhashim et. al. \cite{Alhashim2018}. 


The design of this network is in such a way that the output from the DenseNet block is  fed to a successive series of up-sampling layers. The decoder has 5 Bilinear up-sampling blocks. \(4^{th}\) block is designed in a way that it give the output resolution half of its input, which is 230$\times$320$\times$1 for input size of RGB image 460$\times$640$\times$3. The \(5^{th}\) block has up-sampling factor of 2 which gives us full resolution same as input. In all our experiments we use only 4 blocks and in post processing we up sample by the factor 2 to get full resolution. This saves some computational time. The decoder part is trained on the 120,000 images of NYU v2 depth dataset. Also the decoder does not contain any Batch Normalization or other advanced sub multi task layers as seen in the recent state-of-the-art methods in section \ref{Chapter3:RelatedWork_NNModel}.

Each block of decoder comprises of 4 layers, starting with up-sampling by Bi linear with linear interpolation method. followed by concatenation operation and 2 convolutions 2D layers. The number of filters for these convolutions layer is decided by the number of filter obtained from the encoder layer which can be given by \(D_n =  {E_m} / {2^n}\) where \(D_n\) is number of filters at decoder block \(n\) and \(E_m\) is the number of filter present in the layer which is concatenated from encoder. The kernel size of the 2 convectional layers are of size (3$\times$3). The model proposed by  \cite{Alhashim2018} has target input dimension of 460$\times$640$\times$3 for RGB image and  dimension of 230$\times$320$\times$1 for ground truth label and we acquire the same principles for all our experiments. Also in the results section \ref{Chapter6:Results} we had some experiments by retraining the decodered part with different dataset and configurations to understand the influence of different 

In this approach we also use the transfer learning by obtaining weights which was provided by Alhashim et. al. \cite{Alhashim2018}. 


\section{Experimental Setup}
\label{Chapter5:Experimental_Setup}
\subsection{Evaluation Configurations}

Our entire experimental experimental configuration are described in the fig \ref{fig:Experimental_Setup}. All the configuration where made based on two model \textbf{A1} and \textbf{A2} and two data pre-processing \textbf{Holes} and \textbf{No\_Holes} which gives us 4 configuration to analyze the approach. 


First, Our proposed model \textbf{A1} is a simple U-Net style architecture - also can be categorized as encoder-decoder style of architecture. The reason for this model is to see if a simple network can learn the structural dependencies with respect to its depth. Knowing that the depth images are very much are highly structural dependent we wanted to see how well a network can learn without any prior knowledge. This motivated us to use an existing model to evaluate against the proposed model.We found out the our model performs relatively poor. We used the pre-trained weights of encoder and retrained the decoder layers with new structure dataset. And the results of this transfer leaning makes it more efficient. Hence we have 2 models, a simple U-Net with de-convolutional layers we call it as approach 1  (\textbf{A1}) and another model proposed by Alhashim et. al. \cite{Alhashim2018} and applied transfer learning approach we call it as approach 2 (\textbf{A2}) and \textbf{A2} become the main contribution for good results compared with U-Net approach model. 

Secondly, we wanted to investigate if a model can learn the holes generated from the SLB sensor. In our literature study we where unable to find previous work which was focused to make the network learn the holes and regenerated. The most common and widely used method was to either in-paint (or fill) the holes using neighboring pixel \cite{silberman11indoor} method. We approached this problem by saving the holes and map all the invalid holes to zero value instead of finding neighboring pixel which was used in NYU dataset. We trained \textbf{A2} model in two different fashion. One by mapping the holes to a zero value we call it as \textbf{A2-Holes} and another we find the neighboring pixels and we call this approach as \textbf{A2-NoHoles}. Note that, \textbf{A2-Holes} method might need some pre processing as we will will discuss more in the results section \ref{Chapter6:Results}.



\begin{figure}[h]
    \centering
    \includegraphics[width = 15cm]{Figures/config_setup.png}
    \caption{Different experimental configuration were setup based on \textbf{A1} and \textbf{A2} approaches}
    \label{fig:Experimental_Setup}
\end{figure}{}



Therefore in summary, we have two model \textbf{A1} and \textbf{A2} for validating the proposed model and effects of structural Characteristic. Further more \textbf{A2} model where retrained with two different modified input features for investigating on regeneration of holes. This gives us two more configuration of \textbf{A2} modelnamely \textbf{A2\_Holes} and \textbf{A2\_NoHoles}. 

\section{Evaluation Metrics}
For our evaluation we use standard metrics used in prior work \cite{Alhashim2018, eigen2014depth}. The error metrics are defined as follows:

\begin{itemize}
    
    \item Root Mean Squared Error (RMSE):
    \begin{equation} \label{RMSE}
        \sqrt{\frac{1}{n} \sum_{p}^{n}{(y_{p} - \hat{y}_{p}})^2}
    \end{equation}
    
    \item Average ({log_{10}}) error: 
    \begin{equation} \label{avg_log}
        \frac{1}{n} \sum_{p}^{n} \left|log_{10}(y_{p}) - log_{10}(\hat{y}_{p}) \right|
    \end{equation}
   
    \item Threshold Accuracy (\(\deta_{i}\)): \({\delta = max (\frac{{y_{p}}}{\hat{y_{p}}}, \frac{\hat{y_{p}}}{{y_{p}}})}\)
\end{itemize}{}

\begin{equation} \label{rms}
    {rms =  \frac{1}{n} \sum_{p}^{n} \frac{\left|y_{p} - \hat{y_{p}}\right|}{y}}
\end{equation}



 \({\frac{1}{n} \sum_{p}^{n} \left|log_{10}(y_{p}) - log_{10}(\hat{y_{p}}) \right|}\)

\begin{equation} \label{accuracy}
    {\deta = max (\frac{{y_{p}}}{\hat{y_{p}}}, \frac{\hat{y_{p}}}{{y_{p}}})}
\end{equation}


\newpage






We would like to address that out of the 6 standard error matrics we exclude Average Relative Error (rel): 


\section{Schedule and Milestones}
1 page: Provide a GANTT diagram based on week
