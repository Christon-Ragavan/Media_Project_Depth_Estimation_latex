
\chapter{Methodology}

\label{Chapter5:Methodology} 
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

In this chapter we will discuss the entire work flow and the methods used in this study. Starting from the different model architecture used. We used two two main archetecture and with some configurations to answer our research questions. Also we will be discussing how the structural dataset was created and some pre processing applied in order to investigate on research question.


\section{System Overview}
We approached this problem of depth map estimation from monocular images to give end to end solution as much as possible using two CNN. As we discussed in the section \ref{Chapeter1:Topic_Description} our main focus was (a) to deliver a robust method and (b) regeneration of dead pixels or holes for better reconstruction of images. 

First, in order to deliver a robust network we first investigated and propose a simple model architecture and test against the existing. Our existing model is a simple U-Net style architecture - also can be categorized as encoder-decoder style of architecture. The reason for this model is to see if a simple network can learn the structural dependencies with respect to its depth. Knowing that the depth images are very much are highly structural dependent we wanted to see how well a network can learn without any prior knowledge. This motivated us to use an existing model to evaluate against the proposed model.We found out the our model performs relatively poor. We used the pre-trained weights of encoder and retrained the decoder layers with new structure dataset. And the results of this transfer leaning makes it more efficient. Hence we have 2 models, a simple U-Net with de-convolutional layers we call it as approach 1  (\textbf{A1}) and another model proposed by Alhashim et. al. \cite{Alhashim2018} and applied transfer learning approach we call it as approach 2 (\textbf{A2}) and \textbf{A2} become the main contribution for good results compared with U-Net approach model. 

Secondly, we wanted to investigate if a model can learn the holes generated from the SLB sensor. In our literature study we where unable to find previous work which was focused to make the network learn the holes and regenerated. The most common and widely used method was to either in paint the holes using neighboring pixel \cite{silberman11indoor} or some other methods where used. We approached this problem by saving the holes and map all the invalid holes to zero value instead of finding neighboring pixel which was used in NYU dataset. We trained \textbf{A2} model in two different fashion. One by mapping the holes to a zero value we call it as \textbf{A2-Holes} and another we find the neighboring pixels and we call this approach as \textbf{A2-NoHoles}. Note that, \textbf{A2-Holes} method might need some proprocessing as we will will discuss more in the results section \ref{Chapter6:Results}.

Therefore, we have two model \textbf{A1} and \textbf{A2} for validating the proposed model and effects of structural Characteristic. Further more \textbf{A2} model where retrained with two different modified input features for investigating on regeneration of holes. This gives us two more configuration of \textbf{A2} modelnamely \textbf{A2\_Holes} and \textbf{A2\_NoHoles}. 








\subsection{Approach 1 (A1) - U Net Style Network}

\begin{figure}[t]
    \centering
    \includegraphics[width = 14cm, height = 9cm]{Figures/A1.png}
    \caption{U-Net architecture (\textbf{A1}). The arrow mark shows the concatenation of the higher level learnt feature from encoder to the upsamplng decoder part.}
    \label{fig:A1-U-NetArchetecture}
\end{figure}{}

U-Net (\textbf{A1}) architecture as shown in Fig \ref{fig:A1-U-NetArchetecture} is built upon simple idea of using upsampling layer or de-convolution layer  \footnote{not to be confused with the nomenclature, there are various names for de-convolution such as up-sampling layers or transposed convolution layer used by tensorflow and keras (\url{www.tensorflow.org/})} for up scaling the learnt feature and regeneration of depth maps. Hence the decoder block comprises of transposed 2D convolution layers which have the same dimension as the kernal size of encoder block. This means number of layers in encoder and decoder are the sample that's why the name U-Net. We use \textbf{A1} architecture since its easy to configure and understand how the up sample of network can be done. Another advantage is training such a small network is faster but it is a challenge is the network can learn the low level features. 

\textbf{A1} consists of 4 blocks of encoder and decoder part. Each block of encoder consists of 4 layers, 3 layers of 2D Convolutions and 2D max pooling layer of size 2$\times$2 at the end of each block. We designed the 3 convolutions layer with the increasing order of number of filter 4, 16 and 32 with kernel size of 3$\times$3. In the similar way in each decoder block we have 4 layers, starting with a up-sampling layer with kernel size if 2$\times$2 which means the up scaling of the image is in the factor of 2. Nearest neighbor interpolation method used by the up-sampling layer \footnote{\url{www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D}} implemented by keras layer. Towards the end each decoder block as 3 convolution 2D stacked in the similar way as encoder blocks. Each decoder block is concatenated with relative encoder block with has same dimension as shown in fig. \ref{fig:A1-U-NetArchetecture} with arrow marks. The dimension of encoder and decoder block output tensors are kept in symmetrical. This can be achieved by having same kernel size of max pooling at encoder part and up sampling layer at decoder, hence the symmetry in the emcoder and decoder layers. The last output layer comprises of Convolution 2D layer of 1$\times$1 with sigmoid activation. Also last layer has no stride to keep the dimension same as the input. 
This model has input dimension of (batch size$\times$480$\times$640$\times$3) and output dimension of (480$\times$640$\times$1). 



\subsection{Approach 2 (A2) - DenseNet backbone}

\begin{figure}[h]
    \centering
    \includegraphics[width = 15cm,  height = 10cm]{Figures/A2.png}
    \caption{DenseNet}
    \label{fig:A2-DenseNet-arch}
\end{figure}{}




\textbf{A2} as shown in fig. \ref{fig:A2-DenseNet-arch} shows an overview of our second approach for depth estimation. We adapted the idea proposed by Alhashim et. al. \cite{Alhashim2018}. The basic idea was to use DenseNet as backbone (encoder). The input RGB image is fed to the DenseNet-169 \cite{huang2017densely} network which is pretrained on ImageNet \cite{deng2009imagenet}.  Backbone DenseNet architecture is designed in a feed forward fashion within a dense block or in other words such a a way that each layer is directly connected to every other layer. ImageNet is a large database of images which was in turn build upon WorldNet which is organized in a hierarchical manner, which means the images are trained and clustered according to carious classes which give a good hierarchical tree and sub-tree format for classification or clustering. Another great advantage of this net is its versatility of the classes ranging from mammals, vehicles, birds to furniture with 12 subtree and giving 5247 category at the time of this paper by Deng et. al.\ref{deng2009imagenet}. ImageNet claims to have a order of 50 million images and an average of 5000 image per node \footnote{\url{http://www.image-net.org/}}. ImageNet is also good for object recognition, classification and also clustering problems. Due to such versatility ImageNet can give a us a good generalisation of the physical structure from a 2D image which is very important for our task. This also has proven to give state of the results by the model proposed by Alhashim et. al. \cite{Alhashim2018}. 


The design of this network is in such a way that the output from the DenseNet block is  fed to a successive series of up-sampling layers. The decoder has 5 Bilinear up-sampling blocks. \(4^{th}\) block is designed in a way that it give the output resolution half of its input, which is 230$\times$320$\times$1 for input size of RGB image 460$\times$640$\times$3. The \(5^{th}\) block has up-sampling factor of 2 which gives us full resolution same as input. In all our experiments we use only 4 blocks and in post processing we up sample by the factor 2 to get full resolution. This saves some computational time. The decoder part is trained on the 120,000 images of NYU v2 depth dataset. Also the decoder does not contain any Batch Normalization or other advanced sub multi task layers as seen in the recent state-of-the-art methods in section \ref{Chapter3:RelatedWork_NNModel}.

Each block of decoder comprises of 4 layers, starting with up-sampling by Bilinear with leanera interpolatin method. followed by concatenation operation and 2 convolutions 2D layers. The number of filters for these convolutions layer is decided by the number of filter obtained from the encoder layer which can be given by \(D_n =  {E_m} / {2^n}\) where \(D_n\) is number of filters at decoder block \(n\) and \(E_m\) is the number of filter present in the layer which is concatenated from encoder. The kernel size of the 2 convectional layers are of size (3$\times$3). The model proposed by  \cite{Alhashim2018} has target input dimension of 460$\times$640$\times$3 for RGB image and  dimension of 230$\times$320$\times$1 for ground truth label and we acquire the sample principles.  

In this approach we also use the transfer learning by obtaining weights which was provided by Alhashim et. al. \cite{Alhashim2018}. 



\section{Schedule and Milestones}
1 page: Provide a GANTT diagram based on week
